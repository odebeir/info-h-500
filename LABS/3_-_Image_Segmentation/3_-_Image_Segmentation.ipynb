{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "\n",
    "The main objectives of this module are:\n",
    "\n",
    "* Use & compare different methods of segmentation:\n",
    "    * Histogram-based\n",
    "    * Texture-based\n",
    "    * Region growing\n",
    "* Detect objects and extract object features.\n",
    "* Understand corner detection and basic object recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Histogram segmentation\n",
    "\n",
    "In histogram segmentation, we make the hypothesis that the histogram is composed of distinct separable distributions, and we try to find the best threshold to separate those distributions.\n",
    "\n",
    "The code below uses a default threshold of 127 to segment the image. Modify it to:\n",
    "\n",
    "1. Create a function to compute the optimal threshold for an 8 bit image. Apply on the cameraman image.\n",
    "1. Compute the Otsu threshold for an 8-bit image, by optimizing within variance or inter-class variance for each possible theshold *t*. See [here](http://nbviewer.jupyter.org/github/odebeir/info-h-500-501/blob/ver_2016-2017/04-Image%20segmentation/01-Histogram%20based%20image%20segmentation.ipynb) how to compute the Otsu threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "def manual_threshold(im, T):\n",
    "    return im>T\n",
    "\n",
    "def optimal_threshold(im, T0):\n",
    "    pass # TODO\n",
    "    \n",
    "def otsu_threshold(im):\n",
    "    pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_with_threshold(im, T):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    h = plt.hist(im.flatten(), bins=range(256))\n",
    "    plt.plot([T,T],[0,h[0].max()], 'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread('camera.jpg')\n",
    "\n",
    "T = 127 \n",
    "im_segmented = manual_threshold(im, T)\n",
    "\n",
    "plot_histogram_with_threshold(im, T)\n",
    "\n",
    "# Show original image & segmented binary image\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(im)\n",
    "plt.subplot(1,2,2)\n",
    "imshow(im_segmented)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need more help? You can check the following videos:**\n",
    "\n",
    "* [Manual thresholding](https://www.youtube.com/watch?v=rgKkws_fqz4&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=24)\n",
    "* [Optimal threshold](https://www.youtube.com/watch?v=7B1cs0PARtg&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=25)\n",
    "* [Otsu threshold](https://www.youtube.com/watch?v=mz26bssSLds&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Texture segmentation\n",
    "\n",
    "Texture segmentation uses regional descriptors to segment the image based on the local texture. A simple algorithm is provided below, which:\n",
    "\n",
    "* Extract neighborhoods with the sliding window method\n",
    "* Compute the local maximum on the neighborhood and put it in a \"descriptor\" image\n",
    "* Use Otsu thresholding on the descriptor image to segment it\n",
    "* Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters.rank import entropy\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "def texture_descriptor(N):\n",
    "    e = N.max() # Replace with your descriptor\n",
    "    return e\n",
    "\n",
    "def sliding_window(im, PATCH_SIZE):\n",
    "    output = np.zeros((im.shape[0], im.shape[1]))\n",
    "    for i in range(0, im.shape[0]-PATCH_SIZE[0]+1, PATCH_SIZE[0]):\n",
    "        for j in range(0, im.shape[1]-PATCH_SIZE[1]+1, PATCH_SIZE[1]):\n",
    "            patch = im[i:i+PATCH_SIZE[0], j:j+PATCH_SIZE[1]]\n",
    "            output[i:i+PATCH_SIZE[0], j:j+PATCH_SIZE[1]] = texture_descriptor(patch)\n",
    "    return output\n",
    "\n",
    "# Open zebra image as an 8-bit integer grayscale\n",
    "im = img_as_ubyte(imread(\"zebra.jpg\", as_gray=True))\n",
    "\n",
    "im_descr = sliding_window(im,(120,160))\n",
    "T = threshold_otsu(im_descr)\n",
    "mask = im_descr>T\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(1,3,1)\n",
    "imshow(im)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,3,2)\n",
    "imshow(im_descr)\n",
    "plt.title('Descriptor image')\n",
    "plt.subplot(1,3,3)\n",
    "imshow(im*mask)\n",
    "plt.title('Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above example as a starting point, replace the \"maximum\" texture descriptor by properties from the co-occurrence matrix:\n",
    "* Compute the co-occurrence matrix on the neighborhood (see [greycomatrix](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.greycomatrix)). Test different angles & displacements.\n",
    "* Test different properties (see [greycoprops](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.greycoprops))\n",
    "\n",
    "Try to segment the zebra image as best as you can using those descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need more help? You can check the following videos:**\n",
    "\n",
    "* [Region descriptors](https://www.youtube.com/watch?v=5F9aPiab8PA&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=27)\n",
    "* [Properties of the co-occurrence matrix](https://www.youtube.com/watch?v=NR3vrkZ9tyg&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=28)\n",
    "* [Texture segmentation](https://www.youtube.com/watch?v=b-SGhLu-R0c&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Region growing\n",
    "\n",
    "In region growing algorithms, we start from \"markers\" which act as seed points, and grow the segmented regions from those markers. A well-known region-growing algorithm uses the watershed transform. The example below uses the watershed transform on the cameraman image, with hand-picked markers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import disk\n",
    "import skimage.filters.rank as skr\n",
    "from skimage.segmentation import mark_boundaries, watershed\n",
    "from skimage.io import imread\n",
    "\n",
    "im = imread('camera.jpg')\n",
    "\n",
    "smoothing_factor = 4\n",
    "# Compute the gradients of the image:\n",
    "gradient = skr.gradient(skr.mean(im, disk(smoothing_factor)), disk(1))\n",
    "\n",
    "# Hand-picked markers for the road image\n",
    "markers_coordinates = [\n",
    "    [10,256], # sky\n",
    "    [200,150],# cameraman\n",
    "    [400,20], # grass (left)\n",
    "    [400,450] # grass (right)\n",
    "]\n",
    "\n",
    "markers = np.zeros_like(im)\n",
    "for i,(row,col) in enumerate(markers_coordinates):\n",
    "    markers[row,col] = i+1\n",
    "\n",
    "ws = watershed(gradient, markers)\n",
    "\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(im,cmap=plt.cm.gray);\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(gradient,cmap=plt.cm.gray);\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(ws);\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(mark_boundaries(im,ws));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt this method to work on the road image.\n",
    "\n",
    "Can you find a way to automatically determine the markers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imread('road.jpg')\n",
    "\n",
    "imshow(im)\n",
    "\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use of the watershed transform is to separate overlapping object, as in the image below.\n",
    "\n",
    "* Compute the [distance transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.distance_transform_edt.html#scipy.ndimage.distance_transform_edt) of the image.\n",
    "* Use the result to automatically find good markers.\n",
    "* Use the watershed transform to separate the three objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "im = rgb2gray(imread('separ.png'))==0\n",
    "imshow(im)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need more help? You can check the following videos:**\n",
    "\n",
    "* [Markers for region growing](https://www.youtube.com/watch?v=3e0-rE9zhFk&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=30)\n",
    "* [Watershed transform](https://www.youtube.com/watch?v=GuVp7dlzEKE&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=31)\n",
    "* [Distance transform](https://www.youtube.com/watch?v=s1eABOdNnVg&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object features\n",
    "\n",
    "The next step after segmentation is often to extract object features in order to recognize, classify, or measure information about the objects.\n",
    "\n",
    "Starting from the example below:\n",
    "\n",
    "1. **Extract connected components** (see [label()](http://scikit-image.org/docs/dev/api/skimage.measure.html?highlight=label#skimage.measure.label)) of the shapes image and **display the centroid of the objects** (see [regionprops()](http://scikit-image.org/docs/dev/api/skimage.measure.html?highlight=regionprops#skimage.measure.regionprops))\n",
    "1. For each image label, **extract the coordinates of the contour** (see [find_contours()](http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.find_contours)) and **find the corners** of each objects.\n",
    "1. Suggest a method to **classify the objects** in different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops,find_contours\n",
    "\n",
    "im = (imread('shapes.png')[:,:,0]>0).astype(int) #binarize & cast to integer to make it easier to process later\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need more help? You can check the following videos:**\n",
    "\n",
    "* [Object labeling & features](https://www.youtube.com/watch?v=7bOH4E7c8gU&list=PLI3XOM9BWLSW6vTPxc9ZfSABS31o7HMub&index=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding project - Tumour segmentation\n",
    "\n",
    "The image below is a slice of a brain MRI with a large tumour in it. The goal of this project is to create an algorithm to automatically segment the tumour.\n",
    "\n",
    "Given that the resolution of the image is of 0.115 cm/px in both axis, estimate the area of the tumour (in cmÂ²). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread,imshow\n",
    "%matplotlib inline\n",
    "\n",
    "im = imread('mri_brain.jpg')\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
